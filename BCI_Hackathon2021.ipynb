{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55b3748-d31a-4105-9b28-6a010f25ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Modeling & Preprocessing\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Bidirectional\n",
    "from keras import initializers, Model, optimizers, callbacks\n",
    "from keras.models import load_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from glob import glob\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc580682-f3fb-4c51-a800-b8ebc680ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Paths\n",
    "from glob import glob\n",
    "# EEG package\n",
    "from mne import  pick_types\n",
    "from mne.io import read_raw_edf\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "# Save the model\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6f937b-fbd1-40d4-ace7-bacbd5033ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES = os.listdir()\n",
    "for file in FNAMES:\n",
    "    if \".edf\" not in file:\n",
    "        FNAMES.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcb9fd2-cb33-44fe-9df6-d44f98e2bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import from edf files data and targets in the shape of 3D tensor\n",
    "    \n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        \"\"\"\n",
    "\n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 61\n",
    "    sfreq = 250\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        '''This function generate a tensor for X and append it to the existing X'''\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    print(subj_num)\n",
    "    for i, subj in enumerate(subj_num):\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        displayStep = max(len(subj_num)//10, 1)\n",
    "\n",
    "        if i%displayStep == 0:\n",
    "            print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        #fnames = os.listdir()\n",
    "        #for file in fnames:\n",
    "        #    if \".edf\" not in file:\n",
    "        #        fnames.remove(file)\n",
    "        #if \".ipynb_checkpoints\" in fnames:\n",
    "        #    fnames.remove(\".ipynb_checkpoints\")\n",
    "        #for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            fname = subj\n",
    "            print(fname)\n",
    "            raw = read_raw_edf(subj, preload=True, verbose=False)\n",
    "            try:\n",
    "                ch_names = raw.ch_names\n",
    "            except:\n",
    "                continue\n",
    "            #channels = raw.ch_names\n",
    "            #for i in channels:\n",
    "            #    if i != \"Fc3.\" and i != \"Fcz.\" and i != \"Fc4.\" and i != \"C3..\" and i != \"Cz..\" and i != \"C4..\" and i != \"Cp3.\" and i != \"Cpz.\" and i != \"Cp4.\":\n",
    "            #        raw.drop_channels(i)\n",
    "            #raw.filter(7.5,12)\n",
    "\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 250:\n",
    "                print('{} is sampled at 128Hz so will be excluded.'.format(subj))\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            try:\n",
    "                #### for  mne .18\n",
    "                # events = raw.find_edf_events()\n",
    "                #### for  mne .19\n",
    "                events = mne.find_events(raw, verbose = False)\n",
    "            except:\n",
    "                continue\n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            if \"Difficult\" in fname:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([2]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                      \n",
    "            elif \"Med\" in fname:\n",
    "                \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([1]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                        \n",
    "            elif \"Easy\" in fname:\n",
    "                   \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "    \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y, ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8ec8df-f64d-4e08-a0bd-6f40429f9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "#%%\n",
    "def convert_mesh(X):\n",
    "    \n",
    "    mesh = np.zeros((X.shape[0], X.shape[2], 9, 11))\n",
    "    X = np.swapaxes(X, 1, 2)\n",
    "    \n",
    "    # 1st line\n",
    "    mesh[:, :, 0, 4:7] = X[:,:,21:24]; print('1st finished')\n",
    "    \n",
    "    # 2nd line\n",
    "    mesh[:, :, 1, 3:8] = X[:,:,24:29]; print('2nd finished')\n",
    "    \n",
    "    # 3rd line\n",
    "    mesh[:, :, 2, 1:10] = X[:,:,29:38]; print('3rd finished')\n",
    "    \n",
    "    # 4th line\n",
    "    mesh[:, :, 3, 1:10] = np.concatenate((X[:,:,38].reshape(-1, X.shape[1], 1),\\\n",
    "                                          X[:,:,0:7], X[:,:,39].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('4th finished')\n",
    "    \n",
    "    # 5th line\n",
    "    mesh[:, :, 4, 0:11] = np.concatenate((X[:,:,(42, 40)],\\\n",
    "                                        X[:,:,7:14], X[:,:,(41, 43)]), axis=2)\n",
    "    print('5th finished')\n",
    "    \n",
    "    # 6th line\n",
    "    mesh[:, :, 5, 1:10] = np.concatenate((X[:,:,44].reshape(-1, X.shape[1], 1),\\\n",
    "                                        X[:,:,14:21], X[:,:,45].reshape(-1, X.shape[1], 1)), axis=2)\n",
    "    print('6th finished')\n",
    "               \n",
    "    # 7th line\n",
    "    mesh[:, :, 6, 1:10] = X[:,:,46:55]; print('7th finished')\n",
    "    \n",
    "    # 8th line\n",
    "    mesh[:, :, 7, 3:8] = X[:,:,55:60]; print('8th finished')\n",
    "    \n",
    "    # 9th line\n",
    "    #mesh[:, :, 8, 4:7] = X[:,:,60]; print('9th finished')\n",
    "    \n",
    "    # 10th line\n",
    "    #mesh[:, :, 9, 5] = X[:,:,63]; print('10th finished')\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "#%%\n",
    "def prepare_data(X, y, test_ratio=0.2, return_mesh=True, set_seed=42):\n",
    "    \n",
    "    # y encoding\n",
    "    print(\"y encoding\")\n",
    "    oh = OneHotEncoder()\n",
    "    y = oh.fit_transform(y).toarray()\n",
    "    print(\"Finished y\")\n",
    "    \n",
    "    # Shuffle trials\n",
    "    np.random.seed(set_seed)\n",
    "    trials = X.shape[0]\n",
    "    shuffle_indices = np.random.permutation(trials)\n",
    "    X = X[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    \n",
    "    # Test set seperation\n",
    "    train_size = int(trials*(1-test_ratio)) \n",
    "    X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]\n",
    "    print(\"Finish train and test data\")                                \n",
    "    # Z-score Normalization\n",
    "    def scale_data(X):\n",
    "        shape = X.shape\n",
    "        scaler = StandardScaler()\n",
    "        scaled_X = np.zeros((shape[0], shape[1], shape[2]))\n",
    "        displayStep = max(int(shape[0]/10), 1)\n",
    "        for i in range(shape[0]):\n",
    "            for z in range(shape[2]):\n",
    "                scaled_X[i, :, z] = np.squeeze(scaler.fit_transform(X[i, :, z].reshape(-1, 1)))\n",
    "            if i%displayStep == 0:\n",
    "                print('{:.1%} done'.format((i+1)/shape[0]))   \n",
    "        return scaled_X\n",
    "            \n",
    "    X_train, X_test  = scale_data(X_train), scale_data(X_test)\n",
    "    print(\"Finished scale data\")\n",
    "    if return_mesh:\n",
    "        X_train, X_test = convert_mesh(X_train), convert_mesh(X_test)\n",
    "    print(\"Finished convert mesh\")\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e84761f-4a8f-47e8-8f69-e43b4c09d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P15S1Easy.edf', 'P15S2Easy.edf', 'P08S1Difficult.edf', 'P13S2Medium.edf', 'eeg_preprocessing2.py', '.ipynb_checkpoints', 'Untitled-Copy1.ipynb', 'P01S2DifficultData.edf', 'P03S1Easy.edf', 'P01S1MedData.edf', 'P10S1Difficult.edf', 'P06S2Easy.edf', 'P01S2MediumData.edf', 'P03S1Medium.edf', 'P03S2Difficult.edf', 'P03S2Easy.edf', 'P03S2Medium.edf', 'P04S1Difficult.edf', 'P06S1Easy.edf', 'P06S2Medium.edf', 'P07S2Difficult.edf', 'P07S2Medium.edf', 'P08S1Easy.edf', 'P09S2Medium.edf', 'P10S1Medium.edf', 'P10S2Easy.edf', 'P11S2Easy.edf', 'P13S2Difficult.edf', 'P12S2Easy.edf', 'P14S1Medium.edf', 'P14S2Easy.edf', 'P01S1DifficultData.edf', 'P15S1Medium.edf', 'P12S2Difficulty.edf', 'P01S2EasyData.edf', 'P10S2Medium.edf', 'P02S2Difficult.edf', 'P13S1Difficult.edf', 'P07S1Easy.edf', 'P10S2Difficult.edf', 'P12S1Easy.edf', 'P04S1Medium.edf', 'P12S2Medium.edf', 'P02S1Difficult.edf', 'P02S1Easy.edf', 'P02S2Easy.edf', 'P02S1Medium.edf', 'P03S1Difficult.edf', 'P02S2Medium.edf', 'P04S2Difficult.edf', 'P04S1Easy.edf', 'P04S2Medium.edf', 'P05S1Difficult.edf', 'P04S2Easy.edf', 'P05S1Easy.edf', 'P05S2Difficult.edf', 'P05S1Medium.edf', 'P05S2Easy.edf', 'P05S2Medium.edf', 'P06S1Difficult.edf', 'P06S1Medium.edf', 'P07S1Difficult.edf', 'P06S2Difficult.edf', 'P07S2Easy.edf', 'P07S1Medium.edf', 'P08S1Medium.edf', 'P08S2Difficult.edf', 'P08S2Medium.edf', 'P08S2Easy.edf', 'P09S1Difficult.edf', 'P09S1Easy.edf', 'P09S2Difficult.edf', 'P09S1Medium.edf', 'P09S2Easy.edf', 'P11S1Difficult.edf', 'P10S1Easy.edf', 'P11S1Easy.edf', 'P11S1Medium.edf', 'P11S2Difficult.edf', 'P11S2Medium.edf', 'P12S1Medium.edf', 'P12S1Difficult.edf', 'P13S1Easy.edf', 'P13S1Medium.edf', 'P13S2Easy.edf', 'P14S1Difficult.edf', 'P14S2Difficult.edf', 'P14S1Easy.edf', 'P14S2Medium.edf', 'P15S1Difficult.edf', 'P15S2Medium.edf', 'P15S2Difficult.edf', 'model']\n",
      "working on P15S1Easy.edf, 1.1% completed\n",
      "P15S1Easy.edf\n",
      "working on P01S1MedData.edf, 10.8% completed\n",
      "P01S1MedData.edf\n",
      "working on P06S1Easy.edf, 20.4% completed\n",
      "P06S1Easy.edf\n",
      "working on P13S2Difficult.edf, 30.1% completed\n",
      "P13S2Difficult.edf\n",
      "working on P02S2Difficult.edf, 39.8% completed\n",
      "P02S2Difficult.edf\n",
      "working on P02S2Easy.edf, 49.5% completed\n",
      "P02S2Easy.edf\n",
      "working on P05S1Easy.edf, 59.1% completed\n",
      "P05S1Easy.edf\n",
      "working on P07S2Easy.edf, 68.8% completed\n",
      "P07S2Easy.edf\n",
      "working on P09S1Medium.edf, 78.5% completed\n",
      "P09S1Medium.edf\n",
      "working on P12S1Difficult.edf, 88.2% completed\n",
      "P12S1Difficult.edf\n",
      "working on P15S2Medium.edf, 97.8% completed\n",
      "P15S2Medium.edf\n",
      "y encoding\n",
      "Finished y\n",
      "Finish train and test data\n",
      "0.0% done\n",
      "10.0% done\n",
      "20.0% done\n",
      "30.0% done\n",
      "40.0% done\n",
      "50.0% done\n",
      "60.0% done\n",
      "70.0% done\n",
      "80.0% done\n",
      "90.0% done\n",
      "100.0% done\n",
      "0.1% done\n",
      "10.1% done\n",
      "20.1% done\n",
      "30.0% done\n",
      "40.0% done\n",
      "50.0% done\n",
      "60.0% done\n",
      "70.0% done\n",
      "80.0% done\n",
      "90.0% done\n",
      "100.0% done\n",
      "Finished scale data\n",
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "1st finished\n",
      "2nd finished\n",
      "3rd finished\n",
      "4th finished\n",
      "5th finished\n",
      "6th finished\n",
      "7th finished\n",
      "8th finished\n",
      "Finished convert mesh\n"
     ]
    }
   ],
   "source": [
    "X,y,ch_names = get_data(FNAMES, epoch_sec = 0.625)\n",
    "X_train, y_train, X_test, y_test = prepare_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772faff1-8324-4e78-aa67-7c0158837a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make another dimension, 1, to apply CNN for each time frame.\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae132b99-bdc0-461f-8a77-f5900465f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complicated Model - the same as Zhang`s\n",
    "input_shape = X_train.shape[1:5]\n",
    "lecun = initializers.lecun_normal(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04bc26cd-5703-466a-ace4-70e2638d4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "# TimeDistributed Wrapper\n",
    "def timeDist(layer, prev_layer, name):\n",
    "    return TimeDistributed(layer, name=name)(prev_layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b59a7-5826-4f6e-b961-69b922764fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = timeDist(Conv2D(32, (3,3), padding='same', \n",
    "                    data_format='channels_last', kernel_initializer=lecun), inputs, name='CNN1')\n",
    "x = BatchNormalization(name='batch1')(x)\n",
    "x = Activation('elu', name='act1')(x)\n",
    "x = timeDist(Conv2D(64, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN2')\n",
    "x = BatchNormalization(name='batch2')(x)\n",
    "x = Activation('elu', name='act2')(x)\n",
    "x = timeDist(Conv2D(128, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN3')\n",
    "x = BatchNormalization(name='batch3')(x)\n",
    "x = Activation('elu', name='act3')(x)\n",
    "x = timeDist(Flatten(), x, name='flatten')\n",
    "\n",
    "# Fully connected layer block\n",
    "y = Dense(1024, kernel_initializer=lecun, name='FC')(x)\n",
    "y = Dropout(0.5, name='dropout1')(y)\n",
    "y = BatchNormalization(name='batch4')(y)\n",
    "y = Activation(activation='elu')(y)\n",
    "\n",
    "# Recurrent layers block\n",
    "z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
    "z = LSTM(64, kernel_initializer=lecun, name='LSTM2')(z)\n",
    "\n",
    "# Fully connected layer block\n",
    "h = Dense(1024, kernel_initializer=lecun, activation='elu', name='FC2')(z)\n",
    "h = Dropout(0.5, name='dropout2')(h)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(3, activation='softmax')(h)\n",
    "\n",
    "# Model compile\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703b939-995f-44ac-9d9b-c09786547bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model_all_electrodes_fully_connected\"\n",
    "# Get past models\n",
    "MODEL_LIST = glob('model/'+modelname+'*')\n",
    "print(MODEL_LIST)\n",
    "if MODEL_LIST:\n",
    "    print('A model that already exists detected and loaded.')\n",
    "    model = load_model(MODEL_LIST[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c381f9-96ee-4664-8a37-17596c60b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [callbacks.ModelCheckpoint('./model/'+modelname+'.h5', \n",
    "                                            save_best_only=True, \n",
    "                                            monitor='val_loss'),\n",
    "                 callbacks.EarlyStopping(monitor='acc', patience=20),\n",
    "                 callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50),\n",
    "                 callbacks.TensorBoard(log_dir='./tensorboard_dir/'+modelname, \n",
    "                                       histogram_freq=0, \n",
    "                                       write_graph=True,\n",
    "                                       write_images=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643e264-e99f-4c95-ab03-0cb4a96e70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001), metrics=['acc'])\n",
    "hist = model.fit(X_train, y_train, batch_size=64, epochs=1000, \n",
    "                callbacks=callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f5d23-1a0d-46ea-8d6b-dd8d9c84575d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfConda_1",
   "language": "python",
   "name": "tfconda_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
