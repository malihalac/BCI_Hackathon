{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39af22-eff1-4d70-8242-9f1312956dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Modeling & Preprocessing\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Bidirectional\n",
    "from keras import initializers, Model, optimizers, callbacks\n",
    "from keras.models import load_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from glob import glob\n",
    "from keras import optimizers\n",
    "import h5py\n",
    "\n",
    "# Get Paths\n",
    "from glob import glob\n",
    "# EEG package\n",
    "from mne import  pick_types\n",
    "from mne.io import read_raw_edf\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "# Save the model\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorboard\n",
    "\n",
    "FNAMES = os.listdir()\n",
    "for file in FNAMES:\n",
    "    if \"edf\" not in file:\n",
    "        FNAMES.remove(file)\n",
    "\n",
    "if \"model\" in FNAMES:\n",
    "    FNAMES.remove(\"model\")\n",
    "new_FNAMES = []\n",
    "for file in FNAMES:\n",
    "    if \".edf\" in file:\n",
    "        new_FNAMES.append(file)\n",
    "\n",
    "FNAMES = new_FNAMES\n",
    "        \n",
    "print(FNAMES)\n",
    "nomeschema = 'single'\n",
    "\n",
    "#Choose the electrode to analyze between 0-60\n",
    "#NumeroElettrodoDaAnalizzare = 6\n",
    "val_list = []\n",
    "\n",
    "#Just some variables for the model\n",
    "nnpool = [ 8 ,8 ]\n",
    "llpool = [ 16 , 16]\n",
    "ccpool = [ 32 ]\n",
    "ffpool = [ 32 ]\n",
    "\n",
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import from edf files data and targets in the shape of 3D tensor\n",
    "    \n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        \"\"\"\n",
    "\n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 61\n",
    "    sfreq = 250\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        '''This function generate a tensor for X and append it to the existing X'''\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    print(subj_num)\n",
    "    for i, subj in enumerate(subj_num):\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        displayStep = max(len(subj_num)//10, 1)\n",
    "\n",
    "        if i%displayStep == 0:\n",
    "            print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        #fnames = os.listdir()\n",
    "        #for file in fnames:\n",
    "        #    if \".edf\" not in file:\n",
    "        #        fnames.remove(file)\n",
    "        #if \".ipynb_checkpoints\" in fnames:\n",
    "        #    fnames.remove(\".ipynb_checkpoints\")\n",
    "        #for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            fname = subj\n",
    "            print(fname)\n",
    "            raw = read_raw_edf(subj, preload=True, verbose=False)\n",
    "            try:\n",
    "                ch_names = raw.ch_names\n",
    "            except:\n",
    "                continue\n",
    "            #channels = raw.ch_names\n",
    "            #for i in channels:\n",
    "            #    if i != \"Fc3.\" and i != \"Fcz.\" and i != \"Fc4.\" and i != \"C3..\" and i != \"Cz..\" and i != \"C4..\" and i != \"Cp3.\" and i != \"Cpz.\" and i != \"Cp4.\":\n",
    "            #        raw.drop_channels(i)\n",
    "            #raw.filter(7.5,12)\n",
    "\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 250:\n",
    "                print('{} is sampled at 128Hz so will be excluded.'.format(subj))\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            try:\n",
    "                #### for  mne .18\n",
    "                # events = raw.find_edf_events()\n",
    "                #### for  mne .19\n",
    "                events = mne.find_events(raw, verbose = False)\n",
    "            except:\n",
    "                continue\n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            if \"Difficult\" in fname:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([2]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                      \n",
    "            elif \"Med\" in fname:\n",
    "                \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([1]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                        \n",
    "            elif \"Easy\" in fname:\n",
    "                   \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "    \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y, ch_names\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def prepare_data_single(X, y, test_ratio=0.2, set_seed=42):\n",
    "    # y encoding\n",
    "    oh = OneHotEncoder()\n",
    "    y = oh.fit_transform(y).toarray()\n",
    "    \n",
    "    # Shuffle trials\n",
    "    np.random.seed(set_seed)\n",
    "    trials = X.shape[0]\n",
    "    shuffle_indices = np.random.permutation(trials)\n",
    "    X = X[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    \n",
    "    # Test set seperation\n",
    "    train_size = int(trials*(1-test_ratio)) \n",
    "    X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]\n",
    "                                    \n",
    "    # Z-score Normalization\n",
    "    def scale_data(X):\n",
    "        shape = X.shape\n",
    "        scaler = StandardScaler()\n",
    "        scaled_X = np.zeros((shape[0], shape[1], shape[2]))\n",
    "        displayStep = max(int(shape[0]/10), 1)\n",
    "        for i in range(shape[0]):\n",
    "            for z in range(shape[2]):\n",
    "                scaled_X[i, :, z] = np.squeeze(scaler.fit_transform(X[i, :, z].reshape(-1, 1)))\n",
    "            if i%displayStep == 0:\n",
    "                print('{:.1%} done'.format((i+1)/shape[0]))   \n",
    "        return scaled_X\n",
    "            \n",
    "    X_train, X_test  = scale_data(X_train), scale_data(X_test)\n",
    "    X_train, X_test = np.transpose(X_train,(1,2,0)) , np.transpose(X_test,(1,2,0)) \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "##   GET PREPROCESSED DATA FROM FILE  (OR PREPROCES IT ON THE SPOT)\n",
    "for NumeroElettrodoDaAnalizzare in range(61):\n",
    "    for e_s in [0.0625]: ## ADD HERE DIFFERENT TIME WINDOWS (0.125, 0.25 etc.). INSERT MANUALLY DIFFERENT VALUES EACH RUN I WAS HAVING PROBLEMS ADDING VALUES ALL TOGETHER IN THE LOOP\n",
    "        preprocessed_data_file = 'preprocessed_dataset/dataIMM_epochsec-'+str(e_s)+'_schema_'+nomeschema+'_nsub_'+str(len(FNAMES))+'.h5'\n",
    "        print(preprocessed_data_file)\n",
    "        if os.path.exists(preprocessed_data_file):\n",
    "            try:\n",
    "                NumeroElettrodoDaAnalizzare\n",
    "            except NameError:\n",
    "                print(\"NumeroElettrodoDaAnalizzare not defined\")\n",
    "            else:\n",
    "                print('true')\n",
    "                hf = h5py.File(preprocessed_data_file, 'r')\n",
    "                hf.keys()\n",
    "                asciiList = hf.get('ch_names')\n",
    "                ch_names = [n.decode('utf-8') for n in asciiList]\n",
    "                print(ch_names)\n",
    "                y_train =  np.array(hf.get('y_train'))\n",
    "                y_test =  np.array(hf.get('y_test'))\n",
    "                name = ch_names[NumeroElettrodoDaAnalizzare]\n",
    "                print(name)\n",
    "                X_train =  np.array(hf.get(name+'X_train'))\n",
    "                X_test =  np.array(hf.get(name+'X_test'))\n",
    "                hf.close()\n",
    "        else:\n",
    "            print('preprocessing...')\n",
    "            X,y,ch_names = get_data(FNAMES, epoch_sec=e_s)\n",
    "            X_train, y_train, X_test, y_test = prepare_data_single(X, y)\n",
    "            del X\n",
    "            del y\n",
    "            hf = h5py.File(preprocessed_data_file, 'w')\n",
    "            asciiList = [n.encode(\"ascii\", \"ignore\") for n in ch_names]\n",
    "            hf.create_dataset('ch_names', data=asciiList)\n",
    "            for iii, name in enumerate(ch_names):\n",
    "                try:\n",
    "                    print(f' {iii:02d}',name)\n",
    "                    hf.create_dataset(name+'X_train', data=X_train[iii])\n",
    "                    hf.create_dataset(name+'X_test' , data=X_test[iii] )\n",
    "                except:\n",
    "                    continue\n",
    "            hf.create_dataset('y_train', data=y_train)\n",
    "            hf.create_dataset('y_test' , data=y_test)\n",
    "            hf.keys()\n",
    "            hf.close()\n",
    "\n",
    "    X_train = np.transpose(X_train, (1,0))\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = np.transpose(X_test, (1,0))\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    ## Complicated Model - the same as Zhang`s\n",
    "    input_shape = X_train.shape[1:5]\n",
    "    lecun = initializers.lecun_normal(seed=42)\n",
    "\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    # TimeDistributed Wrapper\n",
    "    def timeDist(layer, prev_layer, name):\n",
    "        return TimeDistributed(layer, name=name)(prev_layer)  \n",
    "\n",
    "    y = x\n",
    "\n",
    "\n",
    "    # Convolutional layers block\n",
    "    if ffpool[0]>0:\n",
    "        for idx, val in enumerate(ffpool):\n",
    "            y = Dense(val, kernel_initializer=lecun, name='full'+str(idx+1))(y)\n",
    "            y = Dropout(0.5, name='dropout0_'+str(idx+1))(y)\n",
    "            y = BatchNormalization(name='batch0_'+str(idx+1))(y)\n",
    "            y = Activation(activation='elu')(y)\n",
    "\n",
    "    z = y\n",
    "\n",
    "    # Recurrent layers block\n",
    "    for idx, val in enumerate(llpool[0:-1]):\n",
    "        z = LSTM(val, kernel_initializer=lecun, return_sequences=True, name='LSTM'+str(idx+1))(z)\n",
    "    z = LSTM(llpool[-1], kernel_initializer=lecun, name='LSTM'+str(len(llpool)))(z)\n",
    "    h = z\n",
    "\n",
    "    # Fully connected layer block\n",
    "    for idx, val in enumerate(nnpool):\n",
    "        h = Dense(val, kernel_initializer=lecun, activation='elu', name='FC'+str(idx+1))(h)\n",
    "        h = Dropout(0.2, name='dropout'+str(idx+1))(h)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(3, activation='softmax')(h)\n",
    "\n",
    "    # Model compile\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "\n",
    "    modelname = \"model_\"+ ch_names[NumeroElettrodoDaAnalizzare] + \"_fully_connected\"\n",
    "    # Get past models\n",
    "    MODEL_LIST = glob('model/'+modelname+'*')\n",
    "    print(MODEL_LIST)\n",
    "    if MODEL_LIST:\n",
    "        print('A model that already exists detected and loaded.')\n",
    "        model = load_model(MODEL_LIST[-1])\n",
    "\n",
    "    callbacks_list = [callbacks.ModelCheckpoint('./model/'+modelname+'.h5', \n",
    "                                                save_best_only=True, \n",
    "                                                monitor='val_loss'),\n",
    "                     callbacks.EarlyStopping(monitor='acc', patience=10),\n",
    "                     callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50),\n",
    "                     callbacks.TensorBoard(log_dir='./tensorboard_dir/'+modelname, \n",
    "                                           histogram_freq=0, \n",
    "                                           write_graph=True,\n",
    "                                           write_images=True)]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001), metrics=['acc'])\n",
    "    hist = model.fit(X_train, y_train, batch_size=64, epochs=1000, \n",
    "                    callbacks=callbacks_list, validation_data=(X_test, y_test))\n",
    "                    \n",
    "    val_list.append([ch_names[NumeroElettrodoDaAnalizzare], max(hist.history[\"acc\"]), max(hist.history[\"val_acc\"])])\n",
    "\n",
    "header = [\"electrode\", \"Train Accuracy\", \"Validation Accuracy\"]\n",
    "import csv\n",
    "with open('accuracies_electrodes.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a878d-a32d-4437-9439-aac4b2824058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858bfb7-2598-48db-af33-a29a7cf4bde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfConda_1",
   "language": "python",
   "name": "tfconda_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
