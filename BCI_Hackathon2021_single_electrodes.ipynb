{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55b3748-d31a-4105-9b28-6a010f25ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Modeling & Preprocessing\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Bidirectional\n",
    "from keras import initializers, Model, optimizers, callbacks\n",
    "from keras.models import load_model\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from glob import glob\n",
    "from keras import optimizers\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc580682-f3fb-4c51-a800-b8ebc680ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Paths\n",
    "from glob import glob\n",
    "# EEG package\n",
    "from mne import  pick_types\n",
    "from mne.io import read_raw_edf\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "# Save the model\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6f937b-fbd1-40d4-ace7-bacbd5033ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P15S1Easy.edf', 'P15S2Easy.edf', 'P08S1Difficult.edf', 'P13S2Medium.edf', 'P01S2DifficultData.edf', 'P03S1Easy.edf', 'P01S1MedData.edf', 'P10S1Difficult.edf', 'P06S2Easy.edf', 'P01S2MediumData.edf', 'P03S1Medium.edf', 'P03S2Difficult.edf', 'P03S2Easy.edf', 'P03S2Medium.edf', 'P04S1Difficult.edf', 'P06S1Easy.edf', 'P06S2Medium.edf', 'P07S2Difficult.edf', 'P07S2Medium.edf', 'P08S1Easy.edf', 'P09S2Medium.edf', 'P10S1Medium.edf', 'P10S2Easy.edf', 'P11S2Easy.edf', 'P13S2Difficult.edf', 'P12S2Easy.edf', 'P14S1Medium.edf', 'P14S2Easy.edf', 'P01S1DifficultData.edf', 'P15S1Medium.edf', 'P12S2Difficulty.edf', 'P01S2EasyData.edf', 'P10S2Medium.edf', 'P02S2Difficult.edf', 'P13S1Difficult.edf', 'P07S1Easy.edf', 'P10S2Difficult.edf', 'P12S1Easy.edf', 'P04S1Medium.edf', 'P12S2Medium.edf', 'P02S1Difficult.edf', 'P02S1Easy.edf', 'P02S2Easy.edf', 'P02S1Medium.edf', 'P03S1Difficult.edf', 'P02S2Medium.edf', 'P04S2Difficult.edf', 'P04S1Easy.edf', 'P04S2Medium.edf', 'P05S1Difficult.edf', 'P04S2Easy.edf', 'P05S1Easy.edf', 'P05S2Difficult.edf', 'P05S1Medium.edf', 'P05S2Easy.edf', 'P05S2Medium.edf', 'P06S1Difficult.edf', 'P06S1Medium.edf', 'P07S1Difficult.edf', 'P06S2Difficult.edf', 'P07S2Easy.edf', 'P07S1Medium.edf', 'P08S1Medium.edf', 'P08S2Difficult.edf', 'P08S2Medium.edf', 'P08S2Easy.edf', 'P09S1Difficult.edf', 'P09S1Easy.edf', 'P09S2Difficult.edf', 'P09S1Medium.edf', 'P09S2Easy.edf', 'P11S1Difficult.edf', 'P10S1Easy.edf', 'P11S1Easy.edf', 'P11S1Medium.edf', 'P11S2Difficult.edf', 'P11S2Medium.edf', 'P12S1Medium.edf', 'P12S1Difficult.edf', 'P13S1Easy.edf', 'P13S1Medium.edf', 'P13S2Easy.edf', 'P14S1Difficult.edf', 'P14S2Difficult.edf', 'P14S1Easy.edf', 'P14S2Medium.edf', 'P15S1Difficult.edf', 'P15S2Medium.edf', 'P15S2Difficult.edf']\n"
     ]
    }
   ],
   "source": [
    "FNAMES = os.listdir()\n",
    "for file in FNAMES:\n",
    "    if \"edf\" not in file:\n",
    "        FNAMES.remove(file)\n",
    "\n",
    "if \"model\" in FNAMES:\n",
    "    FNAMES.remove(\"model\")\n",
    "new_FNAMES = []\n",
    "for file in FNAMES:\n",
    "    if \".edf\" in file:\n",
    "        new_FNAMES.append(file)\n",
    "\n",
    "FNAMES = new_FNAMES\n",
    "        \n",
    "print(FNAMES)\n",
    "nomeschema = 'single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27ddd22-b01f-49b4-b2be-70422d0b50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the electrode to analyze between 0-60\n",
    "NumeroElettrodoDaAnalizzare = 6\n",
    "\n",
    "#Just some variables for the model\n",
    "nnpool = [ 8 ,8 ]\n",
    "llpool = [ 16 , 16]\n",
    "ccpool = [ 32 ]\n",
    "ffpool = [ 32 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fcb9fd2-cb33-44fe-9df6-d44f98e2bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subj_num=FNAMES, epoch_sec=0.0625):\n",
    "    \"\"\" Import from edf files data and targets in the shape of 3D tensor\n",
    "    \n",
    "        Output shape: (Trial*Channel*TimeFrames)\n",
    "        \n",
    "        Some edf+ files recorded at low sampling rate, 128Hz, are excluded. \n",
    "        Majority was sampled at 160Hz.\n",
    "        \n",
    "        epoch_sec: time interval for one segment of mashes\n",
    "        \"\"\"\n",
    "\n",
    "    # To calculated completion rate\n",
    "    count = 0\n",
    "    \n",
    "    # Initiate X, y\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # fixed numbers\n",
    "    nChan = 61\n",
    "    sfreq = 250\n",
    "    sliding = epoch_sec/2 \n",
    "    \n",
    "    # Sub-function to assign X and X, y\n",
    "    def append_X(n_segments, old_x):\n",
    "        '''This function generate a tensor for X and append it to the existing X'''\n",
    "        new_x = old_x + [data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))] for n in range(n_segments)\\\n",
    "                     if data[:, int(sfreq*sliding*n):int(sfreq*sliding*(n+2))].shape==(nChan, int(sfreq*epoch_sec))]\n",
    "        return new_x\n",
    "    print(subj_num)\n",
    "    for i, subj in enumerate(subj_num):\n",
    "        # Return completion rate\n",
    "        count+=1\n",
    "        displayStep = max(len(subj_num)//10, 1)\n",
    "\n",
    "        if i%displayStep == 0:\n",
    "            print('working on {}, {:.1%} completed'.format(subj, count/len(subj_num)))\n",
    "\n",
    "        # Get file names\n",
    "        #fnames = os.listdir()\n",
    "        #for file in fnames:\n",
    "        #    if \".edf\" not in file:\n",
    "        #        fnames.remove(file)\n",
    "        #if \".ipynb_checkpoints\" in fnames:\n",
    "        #    fnames.remove(\".ipynb_checkpoints\")\n",
    "        #for i, fname in enumerate(fnames):\n",
    "            \n",
    "            # Import data into MNE raw object\n",
    "            fname = subj\n",
    "            print(fname)\n",
    "            raw = read_raw_edf(subj, preload=True, verbose=False)\n",
    "            try:\n",
    "                ch_names = raw.ch_names\n",
    "            except:\n",
    "                continue\n",
    "            #channels = raw.ch_names\n",
    "            #for i in channels:\n",
    "            #    if i != \"Fc3.\" and i != \"Fcz.\" and i != \"Fc4.\" and i != \"C3..\" and i != \"Cz..\" and i != \"C4..\" and i != \"Cp3.\" and i != \"Cpz.\" and i != \"Cp4.\":\n",
    "            #        raw.drop_channels(i)\n",
    "            #raw.filter(7.5,12)\n",
    "\n",
    "            picks = pick_types(raw.info, eeg=True)\n",
    "            \n",
    "            if raw.info['sfreq'] != 250:\n",
    "                print('{} is sampled at 128Hz so will be excluded.'.format(subj))\n",
    "                break\n",
    "            \n",
    "            # High-pass filtering\n",
    "            raw.filter(l_freq=1, h_freq=None, picks=picks)\n",
    "            \n",
    "            # Get annotation\n",
    "            try:\n",
    "                #### for  mne .18\n",
    "                # events = raw.find_edf_events()\n",
    "                #### for  mne .19\n",
    "                events = mne.find_events(raw, verbose = False)\n",
    "            except:\n",
    "                continue\n",
    "            # Get data\n",
    "            data = raw.get_data(picks=picks)\n",
    "            \n",
    "            \"\"\" Assignment Starts \"\"\" \n",
    "            if \"Difficult\" in fname:\n",
    "\n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([2]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                      \n",
    "            elif \"Med\" in fname:\n",
    "                \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([1]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "                        \n",
    "            elif \"Easy\" in fname:\n",
    "                   \n",
    "                # Number of sliding windows\n",
    "                n_segments = int((raw.n_times/(epoch_sec*sfreq))*2-1)\n",
    "                \n",
    "                y.extend([0]*n_segments)\n",
    "                X = append_X(n_segments, X)\n",
    "    \n",
    "    X = np.stack(X)\n",
    "    y = np.array(y).reshape((-1,1))\n",
    "    return X, y, ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8ec8df-f64d-4e08-a0bd-6f40429f9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def prepare_data_single(X, y, test_ratio=0.2, set_seed=42):\n",
    "    # y encoding\n",
    "    oh = OneHotEncoder()\n",
    "    y = oh.fit_transform(y).toarray()\n",
    "    \n",
    "    # Shuffle trials\n",
    "    np.random.seed(set_seed)\n",
    "    trials = X.shape[0]\n",
    "    shuffle_indices = np.random.permutation(trials)\n",
    "    X = X[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    \n",
    "    # Test set seperation\n",
    "    train_size = int(trials*(1-test_ratio)) \n",
    "    X_train, X_test, y_train, y_test = X[:train_size,:,:], X[train_size:,:,:],\\\n",
    "                                    y[:train_size,:], y[train_size:,:]\n",
    "                                    \n",
    "    # Z-score Normalization\n",
    "    def scale_data(X):\n",
    "        shape = X.shape\n",
    "        scaler = StandardScaler()\n",
    "        scaled_X = np.zeros((shape[0], shape[1], shape[2]))\n",
    "        displayStep = max(int(shape[0]/10), 1)\n",
    "        for i in range(shape[0]):\n",
    "            for z in range(shape[2]):\n",
    "                scaled_X[i, :, z] = np.squeeze(scaler.fit_transform(X[i, :, z].reshape(-1, 1)))\n",
    "            if i%displayStep == 0:\n",
    "                print('{:.1%} done'.format((i+1)/shape[0]))   \n",
    "        return scaled_X\n",
    "            \n",
    "    X_train, X_test  = scale_data(X_train), scale_data(X_test)\n",
    "    X_train, X_test = np.transpose(X_train,(1,2,0)) , np.transpose(X_test,(1,2,0)) \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e84761f-4a8f-47e8-8f69-e43b4c09d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_dataset/dataIMM_epochsec-0.0625_schema_single_nsub_89.h5\n",
      "preprocessing...\n",
      "['P15S1Easy.edf', 'P15S2Easy.edf', 'P08S1Difficult.edf', 'P13S2Medium.edf', 'P01S2DifficultData.edf', 'P03S1Easy.edf', 'P01S1MedData.edf', 'P10S1Difficult.edf', 'P06S2Easy.edf', 'P01S2MediumData.edf', 'P03S1Medium.edf', 'P03S2Difficult.edf', 'P03S2Easy.edf', 'P03S2Medium.edf', 'P04S1Difficult.edf', 'P06S1Easy.edf', 'P06S2Medium.edf', 'P07S2Difficult.edf', 'P07S2Medium.edf', 'P08S1Easy.edf', 'P09S2Medium.edf', 'P10S1Medium.edf', 'P10S2Easy.edf', 'P11S2Easy.edf', 'P13S2Difficult.edf', 'P12S2Easy.edf', 'P14S1Medium.edf', 'P14S2Easy.edf', 'P01S1DifficultData.edf', 'P15S1Medium.edf', 'P12S2Difficulty.edf', 'P01S2EasyData.edf', 'P10S2Medium.edf', 'P02S2Difficult.edf', 'P13S1Difficult.edf', 'P07S1Easy.edf', 'P10S2Difficult.edf', 'P12S1Easy.edf', 'P04S1Medium.edf', 'P12S2Medium.edf', 'P02S1Difficult.edf', 'P02S1Easy.edf', 'P02S2Easy.edf', 'P02S1Medium.edf', 'P03S1Difficult.edf', 'P02S2Medium.edf', 'P04S2Difficult.edf', 'P04S1Easy.edf', 'P04S2Medium.edf', 'P05S1Difficult.edf', 'P04S2Easy.edf', 'P05S1Easy.edf', 'P05S2Difficult.edf', 'P05S1Medium.edf', 'P05S2Easy.edf', 'P05S2Medium.edf', 'P06S1Difficult.edf', 'P06S1Medium.edf', 'P07S1Difficult.edf', 'P06S2Difficult.edf', 'P07S2Easy.edf', 'P07S1Medium.edf', 'P08S1Medium.edf', 'P08S2Difficult.edf', 'P08S2Medium.edf', 'P08S2Easy.edf', 'P09S1Difficult.edf', 'P09S1Easy.edf', 'P09S2Difficult.edf', 'P09S1Medium.edf', 'P09S2Easy.edf', 'P11S1Difficult.edf', 'P10S1Easy.edf', 'P11S1Easy.edf', 'P11S1Medium.edf', 'P11S2Difficult.edf', 'P11S2Medium.edf', 'P12S1Medium.edf', 'P12S1Difficult.edf', 'P13S1Easy.edf', 'P13S1Medium.edf', 'P13S2Easy.edf', 'P14S1Difficult.edf', 'P14S2Difficult.edf', 'P14S1Easy.edf', 'P14S2Medium.edf', 'P15S1Difficult.edf', 'P15S2Medium.edf', 'P15S2Difficult.edf']\n",
      "working on P15S1Easy.edf, 1.1% completed\n",
      "P15S1Easy.edf\n",
      "working on P06S2Easy.edf, 10.1% completed\n",
      "P06S2Easy.edf\n",
      "working on P06S2Medium.edf, 19.1% completed\n",
      "P06S2Medium.edf\n",
      "working on P13S2Difficult.edf, 28.1% completed\n",
      "P13S2Difficult.edf\n",
      "working on P10S2Medium.edf, 37.1% completed\n",
      "P10S2Medium.edf\n",
      "working on P02S1Difficult.edf, 46.1% completed\n",
      "P02S1Difficult.edf\n",
      "working on P04S2Medium.edf, 55.1% completed\n",
      "P04S2Medium.edf\n",
      "working on P06S1Difficult.edf, 64.0% completed\n",
      "P06S1Difficult.edf\n",
      "working on P08S2Medium.edf, 73.0% completed\n",
      "P08S2Medium.edf\n",
      "working on P10S1Easy.edf, 82.0% completed\n",
      "P10S1Easy.edf\n",
      "working on P13S1Medium.edf, 91.0% completed\n",
      "P13S1Medium.edf\n",
      "working on P15S2Difficult.edf, 100.0% completed\n",
      "P15S2Difficult.edf\n",
      "0.0% done\n",
      "10.0% done\n",
      "20.0% done\n",
      "30.0% done\n",
      "40.0% done\n",
      "50.0% done\n",
      "60.0% done\n",
      "70.0% done\n",
      "80.0% done\n",
      "90.0% done\n",
      "0.0% done\n",
      "10.0% done\n",
      "20.0% done\n",
      "30.0% done\n",
      "40.0% done\n",
      "50.0% done\n",
      "60.0% done\n",
      "70.0% done\n",
      "80.0% done\n",
      "90.0% done\n",
      " 00 Fp1\n",
      " 01 Fz\n",
      " 02 F3\n",
      " 03 F7\n",
      " 04 FT9\n",
      " 05 FC5\n",
      " 06 FC1\n",
      " 07 C3\n",
      " 08 T7\n",
      " 09 CP5\n",
      " 10 CP1\n",
      " 11 Pz\n",
      " 12 P3\n",
      " 13 P7\n",
      " 14 O1\n",
      " 15 Oz\n",
      " 16 O2\n",
      " 17 P4\n",
      " 18 P8\n",
      " 19 CP6\n",
      " 20 CP2\n",
      " 21 FCz\n",
      " 22 C4\n",
      " 23 T8\n",
      " 24 FT8\n",
      " 25 FC6\n",
      " 26 FC2\n",
      " 27 F4\n",
      " 28 F8\n",
      " 29 Fp2\n",
      " 30 AF7\n",
      " 31 AF3\n",
      " 32 AFz\n",
      " 33 F1\n",
      " 34 F5\n",
      " 35 FT7\n",
      " 36 FC3\n",
      " 37 C1\n",
      " 38 C5\n",
      " 39 TP7\n",
      " 40 CP3\n",
      " 41 P1\n",
      " 42 P5\n",
      " 43 PO7\n",
      " 44 PO3\n",
      " 45 POz\n",
      " 46 PO4\n",
      " 47 PO8\n",
      " 48 P6\n",
      " 49 P2\n",
      " 50 CPz\n",
      " 51 CP4\n",
      " 52 TP8\n",
      " 53 C6\n",
      " 54 C2\n",
      " 55 FC4\n",
      " 56 FT10\n",
      " 57 F6\n",
      " 58 AF8\n",
      " 59 AF4\n",
      " 60 F2\n",
      " 61 Status\n"
     ]
    }
   ],
   "source": [
    "##   GET PREPROCESSED DATA FROM FILE  (OR PREPROCES IT ON THE SPOT)\n",
    "for e_s in [0.0625 ]:\n",
    "    preprocessed_data_file = 'preprocessed_dataset/dataIMM_epochsec-'+str(e_s)+'_schema_'+nomeschema+'_nsub_'+str(len(FNAMES))+'.h5'\n",
    "    print(preprocessed_data_file)\n",
    "    if os.path.exists(preprocessed_data_file):\n",
    "        try:\n",
    "            NumeroElettrodoDaAnalizzare\n",
    "        except NameError:\n",
    "            print(\"NumeroElettrodoDaAnalizzare not defined\")\n",
    "        else:\n",
    "            print('true')\n",
    "            hf = h5py.File(preprocessed_data_file, 'r')\n",
    "            hf.keys()\n",
    "            asciiList = hf.get('ch_names')\n",
    "            ch_names = [n.decode('utf-8') for n in asciiList]\n",
    "            print(ch_names)\n",
    "            y_train =  np.array(hf.get('y_train'))\n",
    "            y_test =  np.array(hf.get('y_test'))\n",
    "            name = ch_names[NumeroElettrodoDaAnalizzare]\n",
    "            print(name)\n",
    "            X_train =  np.array(hf.get(name+'X_train'))\n",
    "            X_test =  np.array(hf.get(name+'X_test'))\n",
    "            hf.close()\n",
    "    else:\n",
    "        print('preprocessing...')\n",
    "        X,y,ch_names = get_data(FNAMES, epoch_sec=e_s)\n",
    "        X_train, y_train, X_test, y_test = prepare_data_single(X, y)\n",
    "        del X\n",
    "        del y\n",
    "        hf = h5py.File(preprocessed_data_file, 'w')\n",
    "        asciiList = [n.encode(\"ascii\", \"ignore\") for n in ch_names]\n",
    "        hf.create_dataset('ch_names', data=asciiList)\n",
    "        for iii, name in enumerate(ch_names):\n",
    "            try:\n",
    "                print(f' {iii:02d}',name)\n",
    "                hf.create_dataset(name+'X_train', data=X_train[iii])\n",
    "                hf.create_dataset(name+'X_test' , data=X_test[iii] )\n",
    "            except:\n",
    "                continue\n",
    "        hf.create_dataset('y_train', data=y_train)\n",
    "        hf.create_dataset('y_test' , data=y_test)\n",
    "        hf.keys()\n",
    "        hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae132b99-bdc0-461f-8a77-f5900465f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complicated Model - the same as Zhang`s\n",
    "input_shape = X_train.shape[1:5]\n",
    "lecun = initializers.lecun_normal(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04bc26cd-5703-466a-ace4-70e2638d4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "# TimeDistributed Wrapper\n",
    "def timeDist(layer, prev_layer, name):\n",
    "    return TimeDistributed(layer, name=name)(prev_layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b59a7-5826-4f6e-b961-69b922764fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x\n",
    "\n",
    "\n",
    "# Convolutional layers block\n",
    "if ffpool[0]>0:\n",
    "    for idx, val in enumerate(ffpool):\n",
    "        y = Dense(val, kernel_initializer=lecun, name='full'+str(idx+1))(y)\n",
    "        y = Dropout(0.5, name='dropout0_'+str(idx+1))(y)\n",
    "        y = BatchNormalization(name='batch0_'+str(idx+1))(y)\n",
    "        y = Activation(activation='elu')(y)\n",
    "\n",
    "z = y\n",
    "\n",
    "# Recurrent layers block\n",
    "for idx, val in enumerate(llpool[0:-1]):\n",
    "    z = LSTM(val, kernel_initializer=lecun, return_sequences=True, name='LSTM'+str(idx+1))(z)\n",
    "z = LSTM(llpool[-1], kernel_initializer=lecun, name='LSTM'+str(len(llpool)))(z)\n",
    "h = z\n",
    "\n",
    "# Fully connected layer block\n",
    "for idx, val in enumerate(nnpool):\n",
    "    h = Dense(val, kernel_initializer=lecun, activation='elu', name='FC'+str(idx+1))(h)\n",
    "    h = Dropout(0.2, name='dropout'+str(idx+1))(h)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(3, activation='softmax')(h)\n",
    "\n",
    "# Model compile\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703b939-995f-44ac-9d9b-c09786547bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model_\"+ ch_names[NumeroElettrodoDaAnalizzare] + \"_fully_connected\"\n",
    "# Get past models\n",
    "MODEL_LIST = glob('model/'+modelname+'*')\n",
    "print(MODEL_LIST)\n",
    "if MODEL_LIST:\n",
    "    print('A model that already exists detected and loaded.')\n",
    "    model = load_model(MODEL_LIST[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c381f9-96ee-4664-8a37-17596c60b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [callbacks.ModelCheckpoint('./model/'+modelname+'.h5', \n",
    "                                            save_best_only=True, \n",
    "                                            monitor='val_loss'),\n",
    "                 callbacks.EarlyStopping(monitor='acc', patience=20),\n",
    "                 callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50),\n",
    "                 callbacks.TensorBoard(log_dir='./tensorboard_dir/'+modelname, \n",
    "                                       histogram_freq=0, \n",
    "                                       write_graph=True,\n",
    "                                       write_images=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643e264-e99f-4c95-ab03-0cb4a96e70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001), metrics=['acc'])\n",
    "hist = model.fit(X_train, y_train, batch_size=64, epochs=1000, \n",
    "                callbacks=callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f5d23-1a0d-46ea-8d6b-dd8d9c84575d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfConda_1",
   "language": "python",
   "name": "tfconda_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
